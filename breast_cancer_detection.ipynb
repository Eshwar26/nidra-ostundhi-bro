{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Breast Cancer Detection from Multi-Cancer Dataset\n",
        "\n",
        "This notebook builds a basic model to detect breast cancer from the Multi-Cancer dataset on Kaggle.\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/obulisainaren/multi-cancer/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q kaggle\n",
        "!pip install -q tensorflow\n",
        "!pip install -q pillow\n",
        "!pip install -q matplotlib\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q seaborn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup Kaggle API Credentials\n",
        "\n",
        "**Instructions:**\n",
        "1. Go to your Kaggle account settings: https://www.kaggle.com/settings\n",
        "2. Scroll to 'API' section and click 'Create New API Token'\n",
        "3. This will download `kaggle.json`\n",
        "4. Upload it using the file upload in the next cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload kaggle.json file\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d obulisainaren/multi-cancer\n",
        "!unzip -q multi-cancer.zip -d dataset\n",
        "!ls dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the dataset structure\n",
        "dataset_path = 'dataset'\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(dataset_path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    if level < 2:  # Only show first 2 levels\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths - adjust these based on actual dataset structure\n",
        "# Common structures: dataset/train/breast or dataset/breast/train\n",
        "\n",
        "# Try to find breast cancer images\n",
        "def find_breast_cancer_path(base_path):\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        if 'breast' in root.lower():\n",
        "            print(f\"Found breast cancer data at: {root}\")\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "breast_path = find_breast_cancer_path(dataset_path)\n",
        "\n",
        "# Count images in each category\n",
        "if breast_path:\n",
        "    categories = os.listdir(breast_path)\n",
        "    print(f\"\\nCategories found: {categories}\")\n",
        "    \n",
        "    for category in categories:\n",
        "        cat_path = os.path.join(breast_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            num_images = len([f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            print(f\"{category}: {num_images} images\")\n",
        "else:\n",
        "    print(\"Breast cancer path not found. Please check dataset structure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# If dataset has train/test split, use it. Otherwise, we'll create our own\n",
        "# Adjust the data_dir based on your exploration above\n",
        "data_dir = breast_path if breast_path else 'dataset'  # Update this path based on exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some sample images\n",
        "def plot_sample_images(data_dir, num_samples=9):\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    \n",
        "    categories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "    \n",
        "    for idx, category in enumerate(categories[:num_samples]):\n",
        "        cat_path = os.path.join(data_dir, category)\n",
        "        images = [f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        if images:\n",
        "            img_path = os.path.join(cat_path, images[0])\n",
        "            img = Image.open(img_path)\n",
        "            \n",
        "            plt.subplot(3, 3, idx + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(category)\n",
        "            plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if breast_path:\n",
        "    plot_sample_images(breast_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create Data Generators\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    validation_split=0.2  # 80-20 split\n",
        ")\n",
        "\n",
        "# Only rescaling for validation\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Create generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Print class indices\n",
        "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
        "print(f\"Number of training samples: {train_generator.samples}\")\n",
        "print(f\"Number of validation samples: {validation_generator.samples}\")\n",
        "print(f\"Number of classes: {len(train_generator.class_indices)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model using transfer learning with EfficientNetB0\n",
        "def create_model(num_classes):\n",
        "    # Load pre-trained EfficientNetB0\n",
        "    base_model = EfficientNetB0(\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    \n",
        "    # Freeze base model initially\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Create model\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    return model, base_model\n",
        "\n",
        "# Create the model\n",
        "num_classes = len(train_generator.class_indices)\n",
        "model, base_model = create_model(num_classes)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Setup Callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, reduce_lr]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Plot Training History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    axes[0, 0].set_title('Model Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
        "    axes[0, 1].set_title('Model Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # Precision\n",
        "    axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
        "    axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
        "    axes[1, 0].set_title('Model Precision')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Precision')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Recall\n",
        "    axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
        "    axes[1, 1].set_title('Model Recall')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "val_loss, val_accuracy, val_precision, val_recall = model.evaluate(validation_generator)\n",
        "\n",
        "print(f\"\\n=== Model Evaluation ===\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {val_precision:.4f}\")\n",
        "print(f\"Validation Recall: {val_recall:.4f}\")\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1_score = 2 * (val_precision * val_recall) / (val_precision + val_recall)\n",
        "print(f\"Validation F1-Score: {f1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Confusion Matrix and Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "validation_generator.reset()\n",
        "y_pred = model.predict(validation_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Get class names\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Save the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('breast_cancer_detection_model.h5')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Save model in SavedModel format (recommended for deployment)\n",
        "model.save('breast_cancer_detection_model_savedmodel')\n",
        "print(\"Model saved in SavedModel format!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to predict on new images\n",
        "def predict_image(image_path, model, class_names):\n",
        "    \"\"\"\n",
        "    Predict cancer type from an image\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    \n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "    confidence = predictions[0][predicted_class_idx]\n",
        "    \n",
        "    # Display results\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    \n",
        "    # Show image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2%}\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    # Show probability distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.barh(class_names, predictions[0])\n",
        "    plt.xlabel('Probability')\n",
        "    plt.title('Class Probabilities')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Example usage (uncomment to test with an image)\n",
        "# test_image_path = 'path/to/your/test/image.jpg'\n",
        "# predicted_class, confidence = predict_image(test_image_path, model, class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Test on Random Validation Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on random validation images\n",
        "def test_random_images(data_dir, model, class_names, num_images=6):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Get random images from validation set\n",
        "    categories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "    \n",
        "    for i in range(min(num_images, len(categories))):\n",
        "        # Pick random category and image\n",
        "        category = np.random.choice(categories)\n",
        "        cat_path = os.path.join(data_dir, category)\n",
        "        images = [f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        \n",
        "        if images:\n",
        "            img_name = np.random.choice(images)\n",
        "            img_path = os.path.join(cat_path, img_name)\n",
        "            \n",
        "            # Load and preprocess\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "            img_array = np.array(img_resized) / 255.0\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "            \n",
        "            # Predict\n",
        "            predictions = model.predict(img_array, verbose=0)\n",
        "            predicted_idx = np.argmax(predictions[0])\n",
        "            predicted_class = class_names[predicted_idx]\n",
        "            confidence = predictions[0][predicted_idx]\n",
        "            \n",
        "            # Plot\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"True: {category}\\nPred: {predicted_class}\\nConf: {confidence:.2%}\")\n",
        "            plt.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test on random images\n",
        "test_random_images(data_dir, model, class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unfreeze base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze early layers, unfreeze later layers\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE/10),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "print(\"Model ready for fine-tuning\")\n",
        "print(f\"Total layers: {len(model.layers)}\")\n",
        "print(f\"Trainable layers: {sum([1 for layer in model.layers if layer.trainable])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune the model (uncomment to run)\n",
        "# history_fine = model.fit(\n",
        "#     train_generator,\n",
        "#     epochs=10,\n",
        "#     validation_data=validation_generator,\n",
        "#     callbacks=callbacks,\n",
        "#     verbose=1\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps & Improvements\n",
        "\n",
        "### Model Improvements:\n",
        "- **Different Architectures**: Try ResNet50, DenseNet121, or Vision Transformers\n",
        "- **Hyperparameter Tuning**: Experiment with learning rates, batch sizes, and dropout rates\n",
        "- **Data Augmentation**: Add more augmentation techniques (color jitter, elastic transforms)\n",
        "- **Ensemble Methods**: Combine multiple models for better predictions\n",
        "- **Class Balancing**: If dataset is imbalanced, use class weights or SMOTE\n",
        "\n",
        "### Fine-tuning:\n",
        "- Uncomment the fine-tuning section above to further improve accuracy\n",
        "- Experiment with unfreezing different numbers of layers\n",
        "\n",
        "### Deployment Options:\n",
        "1. **TensorFlow Lite**: Convert for mobile deployment\n",
        "2. **Web API**: Create REST API using Flask/FastAPI\n",
        "3. **Streamlit App**: Build interactive web interface\n",
        "4. **Docker**: Containerize for easy deployment\n",
        "\n",
        "### Model Optimization:\n",
        "- **Quantization**: Reduce model size for faster inference\n",
        "- **Mixed Precision Training**: Speed up training with FP16\n",
        "- **Cross-Validation**: Implement K-fold cross-validation\n",
        "\n",
        "### Evaluation:\n",
        "- **ROC-AUC Curves**: Better evaluation for medical classification\n",
        "- **Grad-CAM**: Visualize what the model is looking at\n",
        "- **Error Analysis**: Analyze misclassified samples\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
